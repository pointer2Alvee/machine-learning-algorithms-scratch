{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737e2245",
   "metadata": {},
   "source": [
    "<b>Logistic-Regression (LogsReg) - Scratch</b> <br>\n",
    "<i>Implementing logistic regression using only NumPy, step-by-step. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96def8f7",
   "metadata": {},
   "source": [
    "<b>requirements</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "619f7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:- pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69f5fe",
   "metadata": {},
   "source": [
    "<b>imports</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce2316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac970ad7",
   "metadata": {},
   "source": [
    "<b>(1) DATA PRE-PROCESSING</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f454045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Data\n",
    "# Features (X) & Dependent-Variable(y)\n",
    "X, y = datasets.make_blobs(centers=3, n_samples=500, n_features=2, shuffle=True, random_state=42, cluster_std = 1.3)\n",
    "\n",
    "# Data-Splitting \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 134)\n",
    "\n",
    "# Data Pre-Processing\n",
    "# Normalize (if needed)\n",
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad1bdc",
   "metadata": {},
   "source": [
    "<b>(2) ML ALGORITHM - SCRATCH</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80864b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression: \n",
    "    \n",
    "    # (2.1) Initiaize Model-Parameters\n",
    "    def __init__(self, learning_rate = 0.001, iters = 1000) -> None:\n",
    "        \"\"\"\n",
    "        Initialize Model-Parameters\n",
    "        \n",
    "        Parameters-Variables: \n",
    "            learning_rate : [param](int) learning_rate of the model\n",
    "            iters         : [param](int) Number of Training Iterations , default = 1000\n",
    "            weights       : [vars](int) weights of the model, initially None\n",
    "            bias          : [vars](int) bais of the model, initially None\n",
    "            \n",
    "        Returns:\n",
    "            Nothing\n",
    "        \"\"\"\n",
    "\n",
    "        self.iters= iters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    \n",
    "    # (2.2) Calculate Sigmoid Function \n",
    "    def sigmoid(self,z) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates the sigmoid , fx_wb = g(wx+b) = g(z) = 1/(1+e^-z), given z\n",
    "        \n",
    "        Parameters: \n",
    "            z : (np.array) depenent-Variables , z=wx+b\n",
    "            \n",
    "        Returns: \n",
    "            The sigmoid function g(z) for values z\n",
    "        \n",
    "        \"\"\"\n",
    "        gz = 1 / (1 + np.exp(-z))\n",
    "        return gz\n",
    "        \n",
    "    \n",
    "    \n",
    "    # (2.3) Calculate Cost/Loss Function\n",
    "    def cross_entropy_loss(self,X,y) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        The loss function for Logistic Regression is log-loss / Cross-Entropy. \n",
    "        Calculate loss for each datapoints. So its a cost calculation\n",
    "            \n",
    "        Parameters: \n",
    "            X : (np.array) Indepenent-Variables (Features-Matrix)\n",
    "            y : (np.array) True labels \n",
    "            \n",
    "        Returns: \n",
    "           The computed cost\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        n = X.shape[0] # rows, needed when used for loop\n",
    "        cost = 0\n",
    "        \n",
    "        # can also be done using for loop, but vectorization is faster : \n",
    "        \n",
    "        # calculate z\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        \n",
    "        # calculate sigmoid, fx_wb  i.e gz\n",
    "        gz = self.sigmoid(z)\n",
    "        \n",
    "        # calculate cost\n",
    "        cost = -np.dot(y, np.log(gz)) - np.dot((1 - y), np.log(1 - gz))\n",
    "        cost /= n\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    \n",
    "    # (2.4) Fit-Model \n",
    "    def fit(self, X, y) -> None:\n",
    "        \"\"\"\n",
    "        Fits andd Trains Model to Data X. After Training gives us the learned\n",
    "        Calculates Gradients and apply gradient descent algorithm\n",
    "        \n",
    "        Parameter :\n",
    "            X : (np.array) Independent-Variable (Features-Matrix) \n",
    "        \n",
    "        Returns : \n",
    "            Nothing\n",
    "        \"\"\"\n",
    "        \n",
    "        n_data, n_features = X.shape\n",
    "        \n",
    "        # init params\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        history = {}\n",
    "        \n",
    "        for i in range(self.iters):\n",
    "            \n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            y_preds = self.sigmoid(z)\n",
    "\n",
    "            # Calc Gradients/derivs\n",
    "            dw = (1/n_data) * np.dot(X.T, (y_preds-y))\n",
    "            db = (1/n_data) * np.sum((y_preds-y))\n",
    "            \n",
    "            # GD algo / backprop : update Params based on derivs\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # printing : \n",
    "            if i % 100 == 0:\n",
    "                cost = self.cross_entropy_loss(X, y)\n",
    "                history[i] = cost\n",
    "                print(f\"Iter\\t{i}\\tCost\\t{cost}\")\n",
    "\n",
    "        return history, self.weights, self.bias\n",
    "    \n",
    "    \n",
    "    # (2.5) Predicted labels ŷ \n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predictions ŷ of the model, calculates the probabilites and the classes based on those probs\n",
    "        \n",
    "        Parameters: \n",
    "            X : (np.array) Indepenent-Variables (Features-Matrix), X_test in this case\n",
    "        \n",
    "        Returns: \n",
    "             ŷ , the pred\n",
    "        \"\"\"\n",
    "        \n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        y_probs = self.sigmoid(z) # the probabilities of being in a particular class\n",
    "        y_preds = [1 if i > 0.5 else 0 for i in y_probs] # separate to classes 0 or 1 based on probs if > or < than 0.5\n",
    "        return np.array(y_probs), np.array(y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186f364",
   "metadata": {},
   "source": [
    "<b>(3) MODEL TRAINING</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48929703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter\t0\tCost\t0.7477317800761581\n",
      "Iter\t100\tCost\t0.5080910033542689\n",
      "Iter\t200\tCost\t0.3665557601894522\n",
      "Iter\t300\tCost\t0.33580207493736225\n",
      "Iter\t400\tCost\t0.31409867807566644\n",
      "Iter\t500\tCost\t0.2982272196637791\n",
      "Iter\t600\tCost\t0.286223167788391\n",
      "Iter\t700\tCost\t0.2768251260067898\n",
      "Iter\t800\tCost\t0.2692447931704441\n",
      "Iter\t900\tCost\t0.26298239735450696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: 0.7477317800761581,\n",
       "  100: 0.5080910033542689,\n",
       "  200: 0.3665557601894522,\n",
       "  300: 0.33580207493736225,\n",
       "  400: 0.31409867807566644,\n",
       "  500: 0.2982272196637791,\n",
       "  600: 0.286223167788391,\n",
       "  700: 0.2768251260067898,\n",
       "  800: 0.2692447931704441,\n",
       "  900: 0.26298239735450696},\n",
       " array([ 5.05153448e-03,  8.66524306e-03,  2.97521836e-02,  1.56091081e-02,\n",
       "         5.12200001e-05, -8.16830112e-06, -6.82816539e-05, -3.03114194e-05,\n",
       "         1.00783931e-04,  4.01512371e-05,  4.49374913e-05,  6.87556054e-04,\n",
       "         4.86594446e-05, -1.04389482e-02,  3.99564326e-06, -7.90925221e-07,\n",
       "        -1.87416173e-06,  6.21585683e-07,  1.20750046e-05,  1.17941004e-06,\n",
       "         5.20685184e-03,  1.10272651e-02,  2.95784037e-02, -1.92611583e-02,\n",
       "         6.59633597e-05, -6.05398322e-05, -1.40768918e-04, -3.30011471e-05,\n",
       "         1.43409816e-04,  3.88102552e-05]),\n",
       " 0.0006780733445365326)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit (Training) \n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=1234)\n",
    "\n",
    "regressor = LogisticRegression(learning_rate=0.00001, iters=1000)\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f690a9",
   "metadata": {},
   "source": [
    "<b>(4) PREDICTION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3a80801",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c104254",
   "metadata": {},
   "source": [
    "<b>(5) EVALUATION-VISUALIZATION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf345156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR classification accuracy: 0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "print(\"LR classification accuracy:\", accuracy(y_test, predictions))\n",
    "\n",
    "# Predicting on Test-Set using the trained  model (by the learned '')\n",
    "#print(f\"\\nPredicted Class-Label : {predictions}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320bd5c",
   "metadata": {},
   "source": [
    "<b>CONCLUSION</b>\n",
    "- The model is performing well for classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
