{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737e2245",
   "metadata": {},
   "source": [
    "<b>Decision-Tree-Regression (DTR) - Scratch</b> <br>\n",
    "<i>Implementing DT regression using only NumPy, step-by-step. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96def8f7",
   "metadata": {},
   "source": [
    "<b>requirements</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example:- pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69f5fe",
   "metadata": {},
   "source": [
    "<b>imports</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2316ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac970ad7",
   "metadata": {},
   "source": [
    "<b>(1) DATA PRE-PROCESSING</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f454045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     x0   x1      x2    x3        x4        y\n",
      "0   800  0.0  0.3048  71.3  0.002663  126.201\n",
      "1  1000  0.0  0.3048  71.3  0.002663  125.201\n",
      "2  1250  0.0  0.3048  71.3  0.002663  125.951\n",
      "3  1600  0.0  0.3048  71.3  0.002663  127.591\n",
      "4  2000  0.0  0.3048  71.3  0.002663  127.461\n"
     ]
    }
   ],
   "source": [
    "# Read Raw Dataset \n",
    "col_names = ['x0', 'x1', 'x2', 'x3', 'x4', 'y']\n",
    "dataset = pd.read_csv(\"../../../datasets/airfoil_self_noise.csv\", skiprows=1, header=None, names=col_names)\n",
    "print(dataset.head(5)) # values of y are real continous numbers , hence we can be sure that its a regression problem\n",
    "\n",
    "\n",
    "# Feature-Matrix (X) & Dependent-Variable(y)\n",
    "X = dataset.iloc[:, :-1].values\n",
    "Y = dataset.iloc[:, -1].values.reshape(-1,1)\n",
    "\n",
    "# Data-Splitting \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=.2, random_state=41) # 21% test size 79% train size\n",
    "\n",
    "\n",
    "# Custom Dataset :-\n",
    "# X, y = datasets.make_regression(n_samples=150, n_features=1, noise=20, random_state=42)\n",
    "# X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=24)\n",
    "\n",
    "# Data Pre-Processing\n",
    "# Normalize (if needed)\n",
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ad1bdc",
   "metadata": {},
   "source": [
    "<b>(2) ML ALGORITHM - SCRATCH</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80864b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### DECISION-TREE NODE ###########\n",
    "class Node():\n",
    "  \n",
    "    def __init__(self, feature_index=None, threshold=None, left_child=None, \n",
    "                 right_child=None, variance_reduction=None, value=None) -> None:\n",
    "        \"\"\"\n",
    "        Describes the node of the decision tree. Based on feature_index and threshold we\n",
    "        define the condition of the decision nodes. Like x2 <= 32 , \n",
    "        here x2 = x feature, with feature_index = 2\n",
    "            32 is the threshold which when satisfied or not we go either left or right child\n",
    "        \n",
    "        Parameters-Variables: \n",
    "            feature_index : [param]() Index of a particular feature col\n",
    "            threshold     : [param]() Threshold of that particular feature col\n",
    "            left_child    : [param]() Left node of the root node\n",
    "            right_child   : [param]() Right node of the root node\n",
    "            variance_reduction     : [param]() Inormation gain\n",
    "            value         : [param]()    Majority class of datapoints in the leafnode\n",
    "\n",
    "        \"\"\"      \n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.variance_reduction = variance_reduction\n",
    "\n",
    "        # for leaf node \n",
    "        self.value = value \n",
    "\n",
    "\n",
    "########### DECISION-TREE CLASS ###########\n",
    "class DecisionTreeRegressor():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2) -> None:\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \n",
    "        Parameter-Variable:\n",
    "            min_samples_split\n",
    "            max_depth\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Root node of the tree / needed to traverse the tree\n",
    "        self.root_node = None \n",
    "\n",
    "        # stopping conditions to see if a node is leafnode or we dont want to traverse deeper\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        \n",
    "    def build_tree(self, dataset, curr_depth=0) -> Node:\n",
    "        \"\"\"\n",
    "        A recursive function to build the binary tree using recursion\n",
    "        \"\"\"\n",
    "        \n",
    "        # Separating Features and labels into two separate variables\n",
    "        X, y = dataset[:,:-1], dataset[:,-1]\n",
    "        n_data, n_features = np.shape(X) # n_data == n_samples same thing\n",
    "        \n",
    "        # split datapoints from dataset into left/right and build the tree until conditions are met\n",
    "        if n_data >= self.min_samples_split and curr_depth <= self.max_depth:\n",
    "            \n",
    "            # Find the best split / best_split is a dicionary\n",
    "            best_split = self.get_best_split(dataset, n_data, n_features)\n",
    "            \n",
    "            # check if info-gain is positive\n",
    "            \n",
    "            if best_split[\"variance_reduction\"]>0: \n",
    "            # if variance_reduction = 0, means node is leaf node or pure node meaning contains data points of only one class\n",
    "            # if variance_reduction = 1, means each class has equal datapoints in that node\n",
    "                # recurrsion left / build left sub tree\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                \n",
    "                # recurrsion right / build right sub tree\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "                \n",
    "                # return decision node / # 5 params as its a decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"],\n",
    "                            left_subtree, right_subtree, best_split[\"variance_reduction\"])\n",
    "                \n",
    "               \n",
    "        # compute and return leaf node\n",
    "        leaf_value = self.calculate_leaf_value(y)\n",
    "        return Node(value=leaf_value) # only 1 param as its a leaf node\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_best_split(self, dataset, n_data, n_features):\n",
    "        \"\"\"\n",
    "        get the best split of data based traversing all features_index and features values\n",
    "        \"\"\"\n",
    "\n",
    "        # dictionary to store the best split\n",
    "        # best_split = {\n",
    "        #     \"feature_index\" : None,                    \n",
    "        #     \"threshold\": None,\n",
    "        #     \"dataset_left\": None,\n",
    "        #     \"dataset_right\": None,\n",
    "        #     \"variance_reduction\": 0\n",
    "            \n",
    "        # }\n",
    "        best_split = {}\n",
    "        max_variance_reduction = -float(\"inf\")\n",
    "        \n",
    "        # Loop over all features\n",
    "        for feature_index in range(n_features): # n_features = no of cols\n",
    "            feature_values = dataset[:, feature_index] # all rows/values of that col/feature\n",
    "            possible_thresholds = np.unique(feature_values) # unique values of each Feature-col\n",
    "            \n",
    "            # Loop over all possible thresholds / unique values of a particular feature\n",
    "            for threshold in possible_thresholds:\n",
    "                # split dataset (data points for left or right subtree) based on curr feature_idx and curr threshold\n",
    "                dataset_left, dataset_right = self.split(dataset, feature_index, threshold) # \n",
    "                \n",
    "                # check if child are not Null\n",
    "                if len(dataset_left)>0 and len(dataset_right)>0 :\n",
    "                    \n",
    "                    # from the split datasets we extract the label column\n",
    "                    y, left_y , right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:,-1] \n",
    "                    \n",
    "                    # compute info gain, gini method\n",
    "                    curr_variance_reduction = self.get_variance_reduction(y, left_y, right_y)\n",
    "                    \n",
    "                    # update the best split if needed, meaning if in the current loops we get the beter info gain , we just update the currest best infomations\n",
    "                    if curr_variance_reduction > max_variance_reduction:\n",
    "                        best_split[\"feature_index\"] = feature_index                        \n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"variance_reduction\"] = curr_variance_reduction\n",
    "                        max_variance_reduction = curr_variance_reduction\n",
    "\n",
    "        # returns best split\n",
    "        return best_split            \n",
    "    \n",
    "    \n",
    "    # split function\n",
    "    def split(self, dataset, feature_index, threshold):\n",
    "        # traverse each-row for each feature and distribute datapoints/ rows accoring to threshold conditon\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold]) \n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index]>threshold]) \n",
    "        return dataset_left, dataset_right\n",
    "    \n",
    "    \n",
    "    # info gain in classification but in regression its variance reuction\n",
    "    def get_variance_reduction(self, parent, l_child, r_child): \n",
    "        weight_l = len(l_child)/ len(parent)\n",
    "        weight_r = len(r_child)/ len(parent)\n",
    "        reduction = np.var(parent) - (weight_l*np.var(l_child) + weight_r*np.var(r_child))\n",
    "        return reduction\n",
    "\n",
    "   \n",
    "    \n",
    "    # calculate the class of the majority items in the leaf node\n",
    "    def calculate_leaf_value(self, Y):\n",
    "        val = np.mean(Y)\n",
    "        return val\n",
    "\n",
    "    # customizely print the tree\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root_node\n",
    "        \n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "                \n",
    "        else:\n",
    "            # preorder traversal\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.variance_reduction)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left_child, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right_child, indent + indent)\n",
    "    \n",
    "            \n",
    "    # fit function (training and learning splitting thresholds and building the best possible tree)\n",
    "    def fit(self, X, Y):\n",
    "        dataset = np.concatenate((X,Y), axis=1)\n",
    "        self.root_node = self.build_tree(dataset) # build the tree based on the best split\n",
    "        \"\"\"\n",
    "        Here like other ML alogs, where we learn weight and bias, here we learn the spliting thresholds\n",
    "        for which we get the best splitted tree\n",
    "        \"\"\"\n",
    "        \n",
    "    # predict when we get a new value by passing in to the best splitted / trained tree\n",
    "    def predict(self, X):\n",
    "        predictions = [self.make_predictions(x, self.root_node) for x in X]\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    # make predictions()\n",
    "    def make_predictions(self, x, tree) :\n",
    "        if tree.value!= None : return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_predictions(x, tree.left_child)\n",
    "        else:\n",
    "            return self.make_predictions(x, tree.right_child)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186f364",
   "metadata": {},
   "source": [
    "<b>(3) MODEL TRAINING</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48929703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Decision Tree: \n",
      "X_0 <= 3150.0 ? 7.132048702017748\n",
      " left:X_4 <= 0.0337792 ? 3.590330569067664\n",
      "  left:X_3 <= 55.5 ? 1.17898999813184\n",
      "    left:X_4 <= 0.00251435 ? 1.614396721819876\n",
      "        left:128.9919833333333\n",
      "        right:125.90953579676673\n",
      "    right:X_1 <= 15.4 ? 2.2342245360792994\n",
      "        left:129.39160280373832\n",
      "        right:123.80422222222222\n",
      "  right:X_0 <= 1250.0 ? 9.970884020498868\n",
      "    left:X_4 <= 0.0483159 ? 6.35527515982486\n",
      "        left:124.38024528301887\n",
      "        right:118.30039999999998\n",
      "    right:X_3 <= 39.6 ? 5.036286657241031\n",
      "        left:113.58091666666667\n",
      "        right:118.07284615384616\n",
      " right:X_4 <= 0.00146332 ? 29.08299210506528\n",
      "  left:X_0 <= 8000.0 ? 11.886497073996964\n",
      "    left:X_2 <= 0.0508 ? 7.608945827689519\n",
      "        left:134.04247500000002\n",
      "        right:127.33581818181818\n",
      "    right:X_4 <= 0.00076193 ? 10.6229193224008\n",
      "        left:128.94078571428574\n",
      "        right:122.40768750000001\n",
      "  right:X_4 <= 0.0229028 ? 5.638575922510643\n",
      "    left:X_0 <= 6300.0 ? 5.985051045988914\n",
      "        left:120.04740816326529\n",
      "        right:114.67370491803278\n",
      "    right:X_4 <= 0.0368233 ? 8.638744793046438\n",
      "        left:113.83169565217393\n",
      "        right:107.6395833333333\n"
     ]
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor(min_samples_split=3, max_depth=3)\n",
    "regressor.fit(X_train, Y_train)\n",
    "print(\"The Decision Tree: \")\n",
    "regressor.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f690a9",
   "metadata": {},
   "source": [
    "<b>(4) PREDICTION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a80801",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c104254",
   "metadata": {},
   "source": [
    "<b>(5) EVALUATION-VISUALIZATION</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf345156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root mean squared error: 4.851358097184457\n"
     ]
    }
   ],
   "source": [
    "print(f\"root mean squared error: {root_mean_squared_error(Y_test, Y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0320bd5c",
   "metadata": {},
   "source": [
    "<b>CONCLUSION</b>\n",
    "- The model is DT from scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
